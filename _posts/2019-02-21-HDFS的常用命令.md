---
layout: post
title: "HDFS的常用命令"
date: 2019-02-21
description: ""
tag: 大数据
---  
### 1、显示当前目录结构
```shell
hadoop fs -ls  [dir]       # 显示当前目录结构
hadoop fs -ls  -R  [dir]   # 递归显示当前目录结构
hadoop fs -ls  /           # 显示根目录下内容
```

### 2、创建目录
```shell
hadoop fs -mkdir [dir]     # 创建目录
hadoop fs -mkdir -p [dir]  # 递归创建目录
```

### 3、删除操作
```shell
hadoop fs -rm [dir]        # 删除文件
hadoop fs -rm -R  [dir]    # 递归删除目录和文件
```

### 4、导入导出操作
```shell
从本地加载文件到HDFS：
hadoop fs -put  [localsrc] [dst]
hadoop fs - copyFromLocal [localsrc] [dst]   # 以上皆可以

从HDFS导出文件到本地：
hadoop fs -get  [dst] [localsrc]
hadoop fs -copyToLocal [dst] [localsrc]   # 以上皆可以，注意区别
```

### 5、 查看文件内容
```shell
hadoop fs -text -e [dir]    # 检测目录和文件是否存在，存在返回值$?为0，不存在返回1
hadoop fs -cat  [dir]       # 查看文件内容

hadoop fs -tail  [dir]      # 显示文件的最后一千字节
hadoop fs -tail -f  [dir]   # 和Linux下一样，会持续监听文件内容变化 并显示文件的最后一千字节
```

### 6、拷贝文件
```shell
hadoop fs -cp [src] [dst]   # 从源目录复制文件到目标目录
```

### 7、移动文件
```shell
hadoop fs -mv [src] [dst]   # 从源目录移动文件到目标目录
```


### 8、统计当前目录下各文件大小
```shell
hadoop fs -du [OPTIONS] [dir]     # 默认单位字节
[OPTIONS]：
    -s : 显示所有文件大小总和，
    -h : 将以更友好的方式显示文件大小（例如 64.0m 而不是 67108864）
```

### 9、 合并下载多个文件
```shell
hadoop fs -getmerge [OPTIONS]
[OPTIONS]:
    -nl                # 在每个文件的末尾添加换行符（LF）
    -skip-empty-file   # 跳过空文件
# Example:
    # 将HDFS上的hbase-policy.xml和hbase-site.xml文件合并后下载到本地的/usr/test.xml
    hadoop fs -getmerge -nl  /test/hbase-policy.xml /test/hbase-site.xml /usr/test.xml
```

### 10、统计文件系统的可用空间信息
```shell
hadoop fs -df -h /   # 统计文件系统的可用空间信息

hadoop fs -setrep [OPTIONS]: <numReplicas> <path>    # 更改文件复制因
[OPTIONS]:
    -R : 更改文件的复制因子。如果 path 是目录，则更改其下所有文件的复制因子
    -w : 请求命令是否等待复制完成
# Example:
      hadoop fs -setrep -w 3 /user/hadoop/dir1
```

### 11、权限控制  
```shell
# 权限控制和Linux上使用方式一致
# 变更文件或目录的所属群组。 用户必须是文件的所有者或超级用户。
hadoop fs -chgrp [-R] GROUP URI [URI ...]
# 修改文件或目录的访问权限  用户必须是文件的所有者或超级用户。
hadoop fs -chmod [-R] <MODE[,MODE]... | OCTALMODE> URI [URI ...]
# 修改文件的拥有者  用户必须是超级用户。
hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ]
```

### 12、文件检测
```shell
hadoop fs -test [OPTIONS]  URI
[OPTIONS]:
    -d：如果路径是目录，返回 0。
    -e：如果路径存在，则返回 0。
    -f：如果路径是文件，则返回 0。
    -s：如果路径不为空，则返回 0。
    -r：如果路径存在且授予读权限，则返回 0。
    -w：如果路径存在且授予写入权限，则返回 0。
    -z：如果文件长度为零，则返回 0。
# Example:
hadoop fs -test -e filename  
```
