---
layout: post
title: "Hive数仓"
date: 2019-02-27
description: ""
tag: 大数据
---
### 1、Hive的基本概念
#### 1.1 前言
Hive 是一个构建在Hadoop之上的数据仓库，它可以将结构化的数据文件映射成表，并提供类SQL查询功能，用于查询的SQL语句会被转化为MapReduce作业，然后提交到Hadoop上运行
#### 1.2 特点
>- 避免了重写MR，简单、容易上手(提供了类似sql的查询语言hql);
>- 灵活性高，可以自定义用户函数(UDF)和存储格式；
>- 为超大的数据集设计的计算和存储能力，集群扩展容易;
>- 统一的元数据管理，可与presto/impala/sparksql等共享数据；
>- 执行延迟高，不适合做数据的实时处理，但适合做海量数据的离线处理。

>- MepReduce数据处理流程的限制，效率更高的算法无法实现；
>- 调优较难，粒度较粗

#### 1.3 架构原理
![clint](/images/posts/hive/hive体系架构.png)
### 2、Hive安装部署
#### 2.1 下载地址
[http://archive.apache.org/dist/hive/](http://archive.apache.org/dist/hive/)
官方文档查看地址：[https://cwiki.apache.org/confluence/display/Hive/GettingStarted](https:/cwiki.apache.org/confluence/display/Hive/GettingStarted)
#### 2.2 安装
将apache-hive-1.2.2安装在/opt/module/下，然后将其改名为hive
#### 2.3 修改配置文件
进入/opt/module/hive/conf目录下，将hive-env.sh.template名称改为hive-env.sh
```shell
 mv hive-env.sh.template hive-env.sh
```
配置HADOOP_HOME路径
```xml
export HADOOP_HOME=/opt/module/hadoop-2.7.7
```
配置HIVE_CONF_DIR路径
```xml
export HIVE_CONF_DIR=/opt/module/hive/conf
```
创建Hive-site.xml文件，主要是配置存放元数据的MySQL的地址、驱动、用户名和密码等信息：
hive-site.xml:
```xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://hadoop101:3306/hadoop_hive?createDatabaseIfNotExist=true</value>
  </property>

  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.jdbc.Driver</value>
  </property>

  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>root</value>
  </property>

  <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>123456</value>
  </property>

  # 显示当前数据库
  <property>
    <name>hive.cli.print.header</name>
    <value>true</value>
  </property>

  # 显示表头信息
  <property>
    <name>hive.cli.print.current.db</name>
    <value>true</value>
  </property>
</configuration>
```
修改/opt/module/hive/conf/hive-log4j.properties.template 文件名称为hive-log4j.properties，修改log存放位置
```bashshell
hive.log.dir=/opt/modulehive/logs
```
#### 2.4 拷贝数据库驱动
将MySQL驱动包mysql-connector-java-5.1.47-bin.jar拷贝到Hive安装目录的lib目录下
#### 2.5 启动hive
启动hive之前一定要先启动hadoop集群(非HA)
```bashshell
# 若之前格式化过namenode，就不需要再次格式化
hadoop101:sbin/start-dfs.sh
hadoop102:sbin/start-yarn.sh
```
```shell
[root@hadoop101 hive]# bin/hive
```
开启成功后可在mysql中看到Hive创建的库和存放元数据信息的表

### 3、常用DDL操作
#### 3.1 简单使用
```shell
show databases;
use default;
desc database [extended] db_name;  --查看数据库信息 extended详细查看

# 默认行为是 RESTRICT，如果数据库中存在表则删除失败。要想删除库及其中的表，可以使用 CASCADE 级联删除。
drop (database|schema) [if exist] database_name [RESTRICT|CASCADE];

# 创建student表, 并声明文件分隔符’\t’
create table student(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';
```

#### 3.2 分区表基本操作
```
CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name     --表名
  [(col_name data_type [COMMENT col_comment],
    ... [constraint_specification])]  --列名 列数据类型
  [COMMENT table_comment]   --表描述
  [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]  --分区表分区规则
  [
    CLUSTERED BY (col_name, col_name, ...)
   [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS
  ]  --分桶表分桶规则
  [SKEWED BY (col_name, col_name, ...) ON ((col_value, col_value, ...), (col_value, col_value, ...), ...)  
   [STORED AS DIRECTORIES]
  ]  --指定倾斜列和值
  [
   [ROW FORMAT row_format]    
   [STORED AS file_format]
     | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)]  
  ]  -- 指定行分隔符、存储文件格式或采用自定义存储格式
  [LOCATION hdfs_path]  -- 指定表的存储位置
  [TBLPROPERTIES (property_name=property_value, ...)]  --指定表的属性
  [AS select_statement];   --从查询结果创建表
```
#### 3.3 分区表操作案例：
分区多种操作
```
# 创建分区表：
create table dept_partition(
deptno int, dname string, loc string
)
partitioned by (month string)
row format delimited fields terminated by '\t';

# 加载数据到分区表中
load data local inpath '/opt/module/datas/dept.txt' into table default.dept_partition partition(month='201709');
load data local inpath '/opt/module/datas/dept.txt' into table default.dept_partition partition(month='201708');
load data local inpath '/opt/module/datas/dept.txt' into table default.dept_partition partition(month='201707');

# 单分区查询
select * from dept_partition where month='201709';

# 多分区联合查询
select * from dept_partition where month='201709' union select * from dept_partition where month='201708' union select * from dept_partition where month='201707';

# 增加分区
alter table dept_partition add partition(month='201706');  -- 增加单个
alter table dept_partition add partition(month='201705') partition(month='201704');  -- 增加多个

# 删除分区
alter table dept_partition drop partition (month='201704');  -- 删除单个
alter table dept_partition drop partition (month='201705'), partition (month='201706'); --删除多个

# 查看分区数
show partitions dept_partition;

# 查看分区结构
desc formatted dept_partition;

```

### 4、数据类型
#### 4.1 基本数据类型

| 大类                                    | 类型                                                         |
| --------------------------------------- | ------------------------------------------------------------ |
| **Integers（整型）**                    | TINYINT—1 字节的有符号整数 <br/>SMALLINT—2 字节的有符号整数<br/> INT—4 字节的有符号整数<br/> BIGINT—8 字节的有符号整数 |
| **Boolean（布尔型）**                   | BOOLEAN—TRUE/FALSE                                           |
| **Floating point numbers（浮点型）**    | FLOAT— 单精度浮点型 <br/>DOUBLE—双精度浮点型                 |
| **Fixed point numbers（定点数）**       | DECIMAL—用户自定义精度定点数，比如 DECIMAL(7,2)               |
| **String types（字符串）**              | STRING—指定字符集的字符序列<br/> VARCHAR—具有最大长度限制的字符序列 <br/>CHAR—固定长度的字符序列 |
| **Date and time types（日期时间类型）** | TIMESTAMP —  时间戳 <br/>TIMESTAMP WITH LOCAL TIME ZONE — 时间戳，纳秒精度<br/> DATE—日期类型 |
| **Binary types（二进制类型）**          | BINARY—字节序列                                              |

#### 4.2 复杂数据类型
Hive有三种复杂数据类型ARRAY、MAP和STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。

在/opt/module/datas/目前中创建test.txt文件，文件内容如下：
```
songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing
yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing
```
案例操作：
```
# 创建测试表
create table test(
name string,
friends array<string>,
children map<string, int>,
address struct<street:string, city:string>
)
row format delimited fields terminated by ','             -- 列分隔符
collection items terminated by '_'                        --MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)
map keys terminated by ':'                                -- MAP 中的 key 与 value 的分隔符
lines terminated by '\n';                                 -- 行分隔符

# 导入文本到测试表中
load data local inpath ‘/opt/module/datas/test.txt’into table test

# 访问三种数据类型的数据：
select friends[1],children['xiao song'],address.city from test where name="songsong";
```

### 5、常用DML操作
#### 5.1 数据导入
```
load data [local] inpath '/opt/module/datas/test.txt' [overwrite] into table tb_name
[partition (partcol1=val1,…)];

# 加载本地文件到hive
load data local inpath '/opt/module/datas/student.txt' into table default.student;

# 上传文件到HDFS
 dfs -put /opt/module/datas/student.txt /user/hive;

# 加载数据HDFS上数据
load data inpath '/user/atguigu/hive/student.txt' into table default.student;

# import数据到指定hive表中
import table student2 partition(month='201709') from '/user/hive/warehouse/export/student';
```

#### 5.2 通过查询导入数据
语法：
```
INSERT OVERWRITE [LOCAL] DIRECTORY directory1
  [ROW FORMAT row_format] [STORED AS file_format]
  SELECT ... FROM ...
```
案例：
```
# 根据查询结果创建表
create table if not exists tb_student2 as select id, name from tb_student1;

# 将查询结果导入到本地
insert overwrite local directory '/opt/module/datas/export/student' select * from student;

# 将查询的结果格式化到处到HDFS上
>insert overwrite local directory '/opt/module/datas/export/student1' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' select * from student;
```

### 在安装部署中遇到的问题总结
```
1、创建表时报错MetaException(message:For direct MetaStore DB connections, we don&#39;t support retries at the client level.)
	解决：将mysql设置为元数据库之后，mysql中会自动创建名为hadoop_hive(非集群的话就是hive)的数据库需将其设置alter database hive character set latin1;

2、drop表的时候，出现卡死状态
	原因：在建好hadoop_hive数据库后,没有第一时间将character_set_database编码由utf8修改为latin1.而是去hive中create了一张表.而后才将character_set_database编码由utf8改成了latin
	解决：在mysql中drop hadoop_hive，重新create hadoop_hive,修改编码alter database hive character set latin1
```
